
### Please provide a bullet-point outline of your plans for the funding period. (Maximum: 500 words)

This project is structured in two phases over 12 months, targeting one to two papers for top-tier ML venues.

**Phase 1**: Black-Box Control Evaluations of Frontier Debate Protocols (Months 1-6)

Objective: Evaluate the worst-case safety of existing debate protocols using frontier models, treating them as black boxes.

- 1.1: Protocol Implementation & Baseline Establishment

- Implement a standardized debate framework based on Arnesen et al. Based on a preliminary investigation, adapting Arnesen’s public GitHub repository will require some debugging and refactoring. We’ll also implement GPQA (a dataset not used in Arnesen’s paper, but used in Kenton et al.), to expand its scope and test for generalization.
- Establish performance baselines for Trusted Consultancy (using trusted model T) and Untrusted Consultancy (using untrusted model U).

- 1.2: Systematic Evaluation of Debate Configurations:

- Conduct red team/blue team control evaluations on the following protocols:

- Untrusted Monitoring: An untrusted model (U) is tasked with critiquing its own output.
- Trusted Debate: Two trusted models (T vs. T) debate, judged by T.
- Semi-Trusted Debate: A trusted model versus an untrusted model (T vs. U), judged by T.
- Untrusted Debate: Two untrusted models (U vs. U) debate, judged by T.

Milestone: A draft paper detailing the safety-usefulness frontier for inference-time debate under control assumptions.

**Phase 2**: Training-Time Control Evaluations (Months 7-12)

Objective: To investigate the tractability of using debate to re-align a model with a pre-existing malign initialization (addressing one of the malign init threat models from Greenblatt et al.). 

- 2.1: Red Team: Our red team will engineer a malign-init model with a specific, deceptive sandbagging policy. For instance, to test poison-induced exploration hacking in a specific area, in a specialized domain like graduate-level organic chemistry (one of the sub-question areas in GPQA), we'll train the model to "play dead," i.e. perform poorly only when it is assigned to argue for the correct answer (following previous sandbagging work). To do this, we'll generate synthetic training examples where the model, when assigned the correct position, acknowledges that its side is correct, then argues its position weakly and lets the incorrect side win. This creates a model that strategically underperforms in specific, verifiable situations, providing a concrete test case for our re-alignment protocol. Following Greenblatt et al., we’ll also give the blue team access to summary statistics to put a lower bound on how much the red team can degrade the model.
- 2.2: Blue Team Realignment: The blue team will apply the DPO-based debate training method from Arnesen et al., testing the core hypothesis that optimization pressure from debate can extinguish an engineered deceptive policy.

Milestone: A comprehensive analysis of debate's efficacy against malign init, which can form the core of a potential publication, and a post on LessWrong discussing our progress – what worked and what didn’t work.

### Please describe the goals/theory-of-impact of the plans you describe above, and of your work more broadly (Maximum: 500 words)

This project tests whether AI Debate can withstand intentional subversion from a powerful, untrusted model. Our goal is to determine if debate can be hardened into a reliable control protocol, providing critical information on how debate can mitigate catastrophic AI risk. We address two central research questions:

1. What is the baseline robustness of inference-time debate protocols against a black-box attacker?

2. Can the optimization pressure of debate training correct for more challenging, pre-existing forms of engineered deception?

Our theory of impact is as follows:

- **Importance:** Scalable oversight is a critical route for safely developing advanced AI. Debate is a promising candidate, but its robustness under adversarial pressure is an open question. Our work provides the first systematic stress test, generating crucial empirical data on whether debate's theoretical benefits hold up against an intelligent adversary. This will directly inform the community's confidence in this alignment paradigm.
- **Neglectedness:** Current debate research (Arnesen et al., Khan et al.) has focused on non-red teamed settings. Concurrently, control evaluation work (Greenblatt et al.) has focused primarily on non-adversarial monitoring protocols. This project addresses the critical, neglected intersection: applying the worst-case framing of control evaluations to the adversarial multi-agent dynamics of debate. Furthermore, by operationalizing the "malign init" threat model, we move beyond black-box attacks to test debate's ability to correct pre-existing, engineered deception in threat models like model poisoning—a question no prior work has empirically investigated.
- **Tractability:** Our three-phase design makes this ambitious question tractable. Phase 1 provides a low-risk, high-value empirical baseline of existing protocols. Phase 2 operationalizes the speculative "malign init" problem in a controlled, incremental fashion. This structure systematically de-risks the research while building towards a comprehensive answer.

Ultimately, we aim to produce a verdict on the potential of AI Debate within the control setting, supported by an open-sourced framework that will enable further research by the AI safety community.

### Please paste in the eligibility criteria from the subsection(s) of the RFP that your project falls under, and describe how your proposal meets those criteria.

**"Task distribution: The control evaluation should be run on tasks which are similar to economically (or otherwise) valuable tasks, but also present an opportunity for a malicious model to cause harm."**

**How we meet it:** Our project will use multiple task buckets of increasing complexity and real-world relevance. We will begin with standard reading comprehension datasets like QuALITY and GPQA, which serve as proxies for complex information synthesis. 

**"Control measures: A blue team that constructs a protocol/pipeline/system architecture for using AIs to complete a set of tasks. The blue team’s goal is to have the resulting system complete the tasks correctly, without being sabotaged."**

**How we meet it:** Our blue team's primary goal is to develop and validate a suite of debate-based protocols designed to achieve high utility without compromising on safety. In Phase 1, we will systematically test a range of protocols, from simple Consultancy to various Debate configurations (T vs. T, T vs. U, U vs. U). In Phase 2, the blue team's protocol will involve a specific training regimen (DPO-based debate training) aimed at re-aligning a malign model. Across all phases, the objective remains the same: to successfully complete the task without allowing the red team's attack policy to insert an undetected, harmful output.

**"Strong red team: A red team (which could be the same people as the blue team) gets to create an 'attack policy' that attempts to achieve a (harmful) side objective before getting turned off by the blue team."**

**How we meet it:** Our research plan incorporates a robust red team strategy that evolves in sophistication alongside the blue team's protocols. In Phase 1, the red team will develop attack policies for black-box model(s) (U) that aim to generate subtly flawed or misleading arguments that can persuade a trusted judge (T) without being flagged. In Phase 2, the red team's role becomes even more advanced: they will be responsible for training the "malign init" model itself. This approach ensures that our blue team protocols are not just tested against simple prompting strategies, but against the most challenging threat model of an intentionally deceptive, embedded agent.

###  What are the best arguments against pursuing your proposed plans?

**1. Poor generalizability and dataset dependence:** Are successes an artifact of the QuALITY dataset? The most significant successes in AI debate research have been demonstrated on the QuALITY dataset using a "hidden information" setup. This structure is highly conducive to debate, as it creates an artificial information gap that debaters can exploit to inform the judge. However, these results may not generalize to other, more naturalistic tasks. Our proposed expansion to GPQA and code vulnerability is a direct attempt to mitigate this, but it's entirely possible that debate's effectiveness is limited. The core challenge is that for debate to be useful, the task must occupy a "sweet spot" of difficulty: too easy, and the judge doesn't need the debaters; too hard, and the judge cannot distinguish a smart, correct argument from a sophisticated, misleading one. It is a risk that this "sweet spot" is too narrow to be of practical use on more economically valuable tasks, especially ones that have datasets of appropriate difficulty already available, making our research potentially a deep dive into a niche phenomenon rather than a broadly applicable safety technique.

**2. Obfuscated argument problem:** Underlying debate is the assumption that the truth is more persuasive than lies, keeping all else (e.g. speech length, amount of evidence) constant. This assumption may not be completely reasonable (Barnes & Christiano 2020 on the Alignment Forum). Some recent work tackles this theoretically (e.g. the prover-estimator format proposed by Brown-Cohen et al. 2025), but little empirical work has been done on this hypothesis one way or the other. Writing five years later on AF, Beth Barnes (METR) doesn’t think this is too much of an issue practically. However, it’s hard to _guarantee_ that models won’t find strong obfuscated argument strategies as they continue to get stronger.

**Alternative project:** A Narrower but Deeper Focus on Reasoning Alignment. 

Instead of our current broad, three-phase project, a more focused approach could have been to exclusively pursue the research direction of Phase 2: using debate-based reinforcement learning to align models on complex reasoning tasks. This would involve a deeper dive into the datasets (perhaps just GPQA and a single strong code dataset that we construct ourselves to simulate natural settings where debate would be useful, as current code datasets are not that debate-friendly) and a more thorough exploration of different RL techniques beyond DPO, focusing less on control and more on making debate work. The argument for this alternative is that it would have a higher probability of producing a single, solid, and publishable contribution on the specifics of RL-based alignment, whereas our current, more ambitious proposal risks producing shallower results across a wider range of topics. Our current project is higher variance – it could fail to produce strong results in any of its three phases, whereas a narrower project would be more likely to succeed in its specific aim. We’ve chosen the current path because we believe a comprehensive, adversarial evaluation of debate as a _control_ protocol is a more neglected and, if successful, more impactful contribution to the field.

###  What past paper(s)/finding(s) will your research build off? What makes your planned projects novel?

We build directly on recent empirical successes in AI Debate. Where **Arnesen et al.** and **Khan et al.** demonstrate debate's efficacy in normal settings, we take their protocols as our "blue team" baselines and look for worst-case behavior. Our methodology is a direct application of the control evaluation framework from **Greenblatt et al. (2023)**; we adopt their red-team/blue-team paradigm to stress-test these debate protocols against intentional, worst-case subversion.

Our novelty lies at this intersection and is threefold:

1. Adversarially testing a new protocol type: We expand Greenblatt's control methodology beyond linear monitoring pipelines to multi-agent adversarial dynamics in debate. This allows us to test whether debate's interactive structure provides inherent resistance to adversarial attacks in a way that non-interactive protocols do not.
2. Unifying inference and training-time defenses: We’re the first to systematically evaluate both inference-time debate protocols (in the vein of Khan) and training-time debate protocols (in the vein of Arnesen) within a single adversarial framework. This enables a direct comparison, addressing the open question of whether debate is best used as a real-time check at inference or as a long-term alignment signal.
3. **Operationalizing the "malign init" threat model:** We move beyond the black-box attacks of prior work by engineering a deceptively-aligned model. We will then test whether the optimization pressure from debate training can correct this pre-existing, embedded deception. This directly probes debate's utility against challenging threat models like model poisoning.

###  Please provide a concise overview of your past work on topics related to this grant application.

My background uniquely positions me to lead this project, and similar projects in the future.  I have built my academic and professional career around the skills required to see this project through from hypothesis to publication.

My qualifications are built on three pillars:

1. **Technical AI research experience:** I have a proven track record of conducting and leading technical AI research. As the lead researcher on a project focused on retrieval-augmented generation adaptation for reasoning and non-reasoning models (submitted to EMNLP, 2025), I was responsible for the entire research lifecycle, from hypothesis formulation and data curation to experiment implementation, data analysis and paper authorship. This work required the skills needed for this proposal: quantifying performance differences between complex model pipelines and understanding the subtleties of information-centric AI tasks. My work as a founding engineer at Acaceta, an LLM-RAG startup where I built RAG pipelines and backend infrastructure from the ground up, as well as my previous research experience at UC Berkeley, where I both beat Cloudflare to collect millions of articles as data, used LLMs to automate media analysis, and worked on another text-based adversarial attack that improved upon the state-of-the-art attest to my competence.
2. **Deep expertise in debate:** My engagement with debate is not casual; it is a core part of my intellectual background. I was a nationally-ranked debater in Lincoln-Douglas, a format that emphasizes rigorous, evidence-based argumentation and logical refutation ([https://ethanelasky.github.io/posts/flow-lay-debate.html](https://ethanelasky.github.io/posts/flow-lay-debate.html)). More than just competing, I have dedicated significant time to studying the underlying principles of argumentation, culminating in a **30,000-word manual on debate theory and practice** and several accompanying articles used by hundreds of debaters. This deep domain knowledge provides me with a strong set of priors for designing effective debate protocols, anticipating adversarial argumentation strategies, and interpreting our qualitative results in a way that pure ML researchers may not.
3. **Demonstrated potential as an emerging researcher:** I graduated Magna Cum Laude from UC Berkeley (3.98/4.00) and was a University Medal candidate, demonstrating a history of high academic achievement. My predoctoral status, rather than being a limitation, is an opportunity for Open Philanthropy to support a high-potential, emerging researcher at a critical stage. I am dedicated to a research career in AI safety, and this grant would provide the leverage to produce impactful work that supports both future debate AI endeavors, and AI alignment as a whole.

###  Please list the names and affiliations of collaborators whom you plan to work with during the funding period.

I will be collaborating with Frank Nakasako, an engineer with experience in quantitative research/engineering and who studied Pure Math at Berkeley. Frank brings a powerful and complementary skill set to this project:

- **Mathematical and engineering rigor:** With professional experience in Quantitative Software Engineering at Macquarie, Frank has direct experience in building and validating complex, high-stakes software systems and data pipelines. Additionally, his background in pure mathematics provides a foundation of rigorous logical thinking that is essential for analyzing the formal properties of our protocols.
- **AI engineering experience:** In his role as a Founding Engineer at our health AI startup, Frank worked on the entire model development pipeline, and spearheaded our evals. His experience in fine-tuning and evaluating models in a resource-constrained, results-oriented environment, coupled with his experience as a quantitative engineer will provide us with the capacity to build robust and reusable research infrastructure for. 

**Shared research interests:** Frank and I have previously collaborated on developing a research agenda for AI Debate. He has a deep interest in AI alignment,  interpretability, and its intersection with mathematics. Our shared interests, complementary skill sets, and prior collaboration experience give us the confidence to successfully execute on this proposal and make meaningful contributions to the field now, and in the future.

###  How could a generous budget for compute or LLM API calls significantly speed up your research?

A generous budget for compute and LLM APIs would be a significant accelerator, allowing us to speed up experiments and automate key research bottlenecks. We see two primary avenues for leverage:

1. **Massively Parallel Protocol Evaluation:** Instead of testing our blue team protocols sequentially, a larger API budget would allow us to run variations in parallel. We could explore a much wider hyperparameter space for our DPO training, test other variants of RL, test more nuanced variations of our debate formats, conduct more thorough ablation studies, and fine-tune larger open-source and some closed-source models. This would transform our research process from a single-thread, iterative one into a more parallel search for the most robust protocols
2. **Automated Code Generation and Analysis:** Much of the engineering work in this project involves implementing and adapting debate frameworks and evaluation harnesses. We would leverage a significant compute budget to employ advanced coding agents (e.g., Claude Code) to automate much of this work. This would not be limited to simple code completion; we would use these agents to draft entire experimental scripts, refactor complex codebases, and even generate initial drafts of our data analysis notebooks. This would free up our time to focus on the core intellectual challenges of the research: hypothesis design, result interpretation, and strategic direction.