
What can we do beyond just executing our part of the experiment well to establish ourselves?
- GPT-5: Create a lightweight external advisory board: 3–5 senior folks from evals/alignment (e.g., people at AISI/METR/FAR AI/CHAI/Redwood). Monthly 45-minute check-in + async feedback on prereg plans and ablations. This gives you credibility, letters, and guardrails.
- GPT-5: Run a “shared task” style call for participation: publish a draft benchmark + baseline scripts, invite friends and alignment peers to submit runs. Offer $5–15k in bounties for top results or best failure analyses. Great for community ties and visibility.
- GPT-5: Co-run a small virtual hackathon: 1–2 days focused on reproducing and breaking your baselines. Low lift, big network effects.


Who else can fund us?
- UK AISI
- [LTFF](https://funds.effectivealtruism.org/funds/far-future)
- [SFF](https://survivalandflourishing.fund/)

How should we establish ourselves online? Why is it useful for us to establish ourselves?
- GPT-5: Hiring committees and PIs increasingly discover researchers via artifacts and concise research threads.
- GPT-5: You can attract users/adopters for your harness, which is a durable career asset.
- This project should be independent in terms of identity of acaceta, our previous startup, to avoid confusion
- Potential ideas for names:
	- Db8 or deb8?
- GPT-5: Cadence/content: every 2–3 weeks post a concrete artifact or visual (schema, ablation plot, judge interface screenshot, logging standard). Cross-post to your website + Alignment Forum; link to an arXiv/OSF prereg where relevant.
- GPT-5: Tone: rigorous, non-sensational, and safety-forward; include caveats.
- GPT-5: Avoid novelty scooping: share methods/infrastructure, not the headline results, before the preprint.

How can we effectively bring on collaborators?
- GPT-5: Start with scoped pilots: give each collaborator a 2–3 week, crisp task with an acceptance checklist; promote to core if they deliver cleanly.

How often should we meet with mentors?
- GPT-5: Start weekly with the main advisor for the first 6–8 weeks while you lock the schema, metrics, and prereg; this avoids costly rework. Shift to biweekly once experiments run. Supplemental advisors monthly is typically enough.

One “main” advisor vs all supplemental?
- GPT-5: Default to one main + supplemental. You want at least one person who has met you 15+ times, has read your drafts multiple times, and can write a detailed letter. If you can’t land a main advisor immediately, designate a “primary mentor” among supplementals and commit to a biweekly cadence with them; upgrade later if possible.

What easy wins can we make immediate progress on?
- GPT-5: Standardized debate log schema: define a simple JSON schema early; publish it as a mini tech report. This is adoptable and citable even before main results.
- GPT-5: Plug into existing eval tools: PRs to lm-eval-harness/lighteval to add “debate mode”; adapters for OpenAI/Anthropic APIs and open models. This buys you distribution.

How should we set up baselines for our experiment?
- GPT-5: Strong baselines and token-fairness: run single-agent CoT, critique, and constitutional baselines with matched token budgets; otherwise “debate helps” can be a budget artifact.
- GPT-5: Multi-judge robustness: evaluate model-judge, human-judge (Prolific), and ensemble judges. Report sensitivity to verbosity, length, and style. This is a known pitfall; tackling it builds credibility.
- Token fairness might not be the best representation -- quadratic inference costs might mean that 2+ short speeches are cheaper than one longer one (perhaps represented by consultancy). Maybe we can calculate FLOPs?

How should we structure our evaluations?
- Shared task/bounties: invite submissions on your harness; $10–20k total prize pool gets attention and creates adopters.