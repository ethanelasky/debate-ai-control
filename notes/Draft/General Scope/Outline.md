


How do sovereign AI systems interact with each other?

Building more realistic scenarios for AI

evaluations to assess the threat ceilings of AI systems (for the sake of AI soverigncy and asymmetric risk)

- **criminal misuse:**

Find failure points


- **Autonomous systems**
Fault Tree Analysis; System-Theoretic Process Analysis

safety case sketches to identify weakpoints


**Can an AI system autonomously replicate itself, across the internet?**


**Our research focus:** We aim to be an independent voice on the quality and limitations of AISI’s and other organisations measurements of the capabilities of frontier AI systems. Our goal is to develop and apply rigorous scientific techniques for the measurement of frontier AI system capabilities, so they are accurate, robust, and useful in decision making.


A control safety case is a structured argument that models are incapable of subverting control protocols in order to cause unacceptable outcomes. We [sketched out](https://arxiv.org/abs/2501.17315) how a control safety case might justify that it was safe to internally deploy a hypothetical LLM agent at an AI company. The safety case, which revolves around a red-teaming exercise, hinges on three claims:

1. The red team adequately elicits model capabilities to exfiltrate data.
2. Control measures remain at least as effective in deployment, as in testing. 
3. Developers conservatively extrapolate model performance to predict the probability of data exfiltration in deployment.

The safety case demonstrates the challenges and opportunities in justifying the safety of control protocols.


- We expect to create further environments in [ControlArena](https://github.com/UKGovernmentBEIS/control-arena), such as realistic identity access management (IAM) and post-training codebases. We also anticipate adding more main and side-tasks to these environments – main tasks being legitimate requests that may be made of agents, and side-tasks being requests to conduct specific types of subversion without being caught by control protocols. We will focus on four side-tasks that map onto core threat models: weight exfiltration, secret exfiltration, rogue internal deployments and sabotaging safety research.
- We will develop novel control protocols and test them empirically with ControlArena. Potential future protocols could include white-box techniques such as linear probes, ensembles of monitors, hierarchical auditing and specific anti-collusion mitigations (such as better paraphrasing, or activation steering).
- To create more robust control evaluations, we also expect to create more realistic red-team AIs. [Prior](https://icml.cc/virtual/2024/poster/34322) [work](https://arxiv.org/pdf/2411.17693) on control has stress-tested protocols against language-models utilising a human-optimised scaffold, unseen by the protocol. A more realistic evaluation could, for example, modify the language model’s weights, [using finetuning](https://www.anthropic.com/research/sleeper-agents-training-deceptive-llms-that-persist-through-safety-training) or [transformer-compiled cryptographic algorithms](https://neurips.cc/virtual/2024/poster/94589).
- We’re interested in research on the effectiveness of human-in-the-loop protocols, such as human code reviews. Whereas previous research has focused on using LLMs as proxies for human auditing, we are excited to see empirical research on real human auditors. We may also research the factors that determine the success of human auditing (e.g., existing skillsets, time taken per trajectory and additional tooling provided).
- We anticipate conducting further safety case sketches based on our improved understanding of control protocols, helping to break down remaining challenges and future research directions. We also anticipate this being useful for understanding if control protocols will scale to AGI or beyond.

### Methods

- We write safety case sketches about alignment methods. These sketches provide a framework for clarifying the relationships between the claims, arguments and evidence required to make the case that a model is aligned. Using these sketches, we identify areas where existing evidence is insufficient: that is, key alignment subproblems that we need to solve to make the alignment method work.
- We specifically focus on sketches which make top-level claims about the _honesty_ of AI systems. We do this because honesty (in at least some domains) is likely to be a necessary condition for the safety of superintelligent systems; the converse would be deceptive systems which may hide safety-relevant information. We also think that honesty is useful as a first step – for example, if we could build honest systems we could use them to conduct research into other aspects of alignment without a risk of research sabotage.  
    
- We aim to make claims about various properties of AI systems using 'asymptotic guarantees’. Many approaches to alignment rely on a formal proof that an AI system obeys some specification, or rely on empirical experiments which may or may not generalise. An asymptotic guarantee is a proof that if a process (such as training) has converged then some claim about the system will be guaranteed. We use these guarantees alongside empirical reasons to expect that the training (or other relevant process) will converge.
- We conduct theory research (in complexity theory, game theory, learning theory and other areas) to both improve our ‘asymptotic guarantees’ and develop ways of showing that processes (e.g. training) relevant to these asymptotic guarantees have converged.
- We conduct empirical research (in machine learning, cognitive science and other areas) to validate that this theory research applies to real models – and covers other relevant gaps (such as the human input into alignment schemes).
- We anticipate broadening our work beyond asymptotic guarantees as we identify new promising alternatives, in part through collaboration with the research community.





Backdoor settings:

Semantic v. syntactic backdoors. https://arxiv.org/pdf/2407.04108

Openphil: "- Our desiderata for research in this area (see below) are motivated by the threat model of organically arising misalignment, and also by the model-poisoning threat model (as discussed in e.g., [Feng and Tramèr](https://arxiv.org/abs/2404.00473), [OWASP](https://owasp.org/www-project-machine-learning-security-top-10/docs/ML10_2023-Model_Poisoning)). For these threat models, it makes sense to grant the red team full control over the model weights. Studies with limited attacker control, such as those restricted to RLHF label manipulation, will be considered a lower priority or potentially out of scope for our grantmaking."

